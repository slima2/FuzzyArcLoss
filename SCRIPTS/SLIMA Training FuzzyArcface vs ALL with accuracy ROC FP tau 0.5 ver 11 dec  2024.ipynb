{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0450cfe4-55b3-4c78-85f1-54d136b80ed0",
   "metadata": {},
   "source": [
    "TWO ADVANTAGES OF FUZZYARCFACE OVER OTHER ANGULAR ALGORITHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f77f82-a48f-44a9-9ec9-8f1d24ce7d12",
   "metadata": {},
   "source": [
    "1. Adaptive Margin Control\n",
    "In most loss functions like ArcFace or SphereFace, the angular margin is either static (fixed for all samples) or is class-specific (same for all samples within a class). However, FuzzyArcFace introduces the concept of a fuzzy membership function to dynamically adjust the margin for each sample. This is a significant innovation, as it allows the network to adjust its decision boundary on a per-sample basis.\n",
    "\n",
    "Key Points of Adaptive Margin Control:\n",
    "Per-Sample Margin Adjustment: Traditional margin-based loss functions (e.g., ArcFace) apply a fixed margin to all samples, regardless of the confidence or quality of the input data. This approach can lead to misclassifications for samples that are difficult or noisy because they are treated the same way as easy-to-classify samples.\n",
    "\n",
    "FuzzyArcFace, on the other hand, applies a fuzzy membership function to each sample, which calculates the confidence or quality of the sample based on its similarity to the class prototype. For highly confident samples (e.g., clear, easy-to-recognize faces), the margin is kept high, ensuring strong intra-class compactness. For uncertain or noisy samples (e.g., occluded or blurry faces), the margin is reduced, allowing the model more flexibility in classifying these harder cases.\n",
    "Dynamic Adjustment of Decision Boundaries: The fuzzy membership function essentially adjusts the decision boundary dynamically. If the model is unsure about a particular sample (e.g., a face that appears very different from the class prototype due to occlusion), it won’t apply the same strict margin, thereby reducing the risk of misclassification. Conversely, for very clear samples, the model can apply a larger margin, pushing the boundaries further and ensuring greater separation from other classes.\n",
    "\n",
    "This leads to adaptive decision boundaries, where the model can fine-tune how much it “trusts” a sample's similarity to its class, thereby avoiding over-confident mistakes that can happen in traditional models with fixed margins.\n",
    "Example:\n",
    "Consider two face images:\n",
    "\n",
    "Image A: A high-quality, front-facing, well-lit face of a person who has been seen many times in training.\n",
    "Image B: A low-quality, side-facing, poorly-lit face of the same person, possibly occluded by sunglasses.\n",
    "In traditional ArcFace or SphereFace, both Image A and Image B would have the same angular margin applied. This can lead to errors, as Image B is much harder to classify and may not fit neatly within the margin's boundary.\n",
    "\n",
    "With FuzzyArcFace, the margin for Image B would be dynamically reduced, allowing more flexibility in how the model interprets this sample. For Image A, a larger margin can be applied, ensuring tighter clustering of confident samples. This adaptive control ensures that the model can handle both easy and difficult samples more effectively.\n",
    "\n",
    "Advantages Over Other Methods:\n",
    "ArcFace: The margin is fixed for all samples, which can be too strict for hard samples and too loose for easy samples.\n",
    "SphereFace2: Although SphereFace2 applies a multiplicative margin, it still treats all samples within a class the same. FuzzyArcFace provides finer control by adjusting the margin on a per-sample basis.\n",
    "AdaptiveFace: While AdaptiveFace adjusts the margin based on class difficulty (e.g., smaller classes get larger margins), it doesn’t adapt to individual sample quality within the class. FuzzyArcFace does both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5fed61-8bf2-46bf-bc86-bc088c7c096a",
   "metadata": {},
   "source": [
    "2. Handling Imbalanced Data\n",
    "Handling class imbalances in training datasets is one of the most significant challenges in deep learning. In many face recognition datasets, certain identities have far fewer images (underrepresented classes), while others have thousands of images (overrepresented classes). This imbalance can lead to overfitting or poor generalization, especially for underrepresented classes.\n",
    "\n",
    "Key Points on Handling Imbalanced Data:\n",
    "Smaller Margins for Underrepresented Classes: In scenarios where a class has very few samples (e.g., a person appears in only a few images in the training set), it becomes harder for the model to generalize and create a well-defined feature space for that class. Most traditional loss functions (e.g., ArcFace) apply the same margin to all classes, which can be problematic for underrepresented classes.\n",
    "\n",
    "FuzzyArcFace addresses this by adjusting the margin based on the fuzzy membership for each sample. If the model is less confident about a sample from an underrepresented class (e.g., a class with only a few samples), the fuzzy membership function reduces the margin, allowing the model more flexibility in correctly classifying these samples. This prevents the model from forcing underrepresented classes to fit into a rigid decision boundary, which could lead to overfitting on a few examples.\n",
    "Prevents Overfitting to Outliers: In imbalanced datasets, there is a risk of overfitting to outlier examples in underrepresented classes. FuzzyArcFace, by reducing the margin for underrepresented or uncertain samples, prevents the model from creating too narrow a decision boundary that is overly influenced by noisy or unusual samples.\n",
    "\n",
    "For example, if a class only has 5 images and 1 of them is an extreme outlier (e.g., a blurred image), a fixed-margin loss function may overfit to this outlier. By dynamically reducing the margin for this uncertain sample, FuzzyArcFace can prevent the model from being overly influenced by the outlier.\n",
    "Example:\n",
    "Consider two classes in a face dataset:\n",
    "\n",
    "Class X: An overrepresented class with 1000 images, featuring a variety of angles, lighting, and expressions.\n",
    "Class Y: An underrepresented class with only 5 images, all in similar lighting and angle.\n",
    "For Class X, the model can learn a well-defined feature space and apply a strict margin to ensure tight intra-class clustering and good separation from other classes. However, for Class Y, applying the same strict margin could force the model to overfit to the limited examples, making it highly sensitive to noise.\n",
    "\n",
    "With FuzzyArcFace, the model applies a smaller margin to Class Y, allowing for a more flexible decision boundary that accommodates the limited variability in the available samples. This prevents overfitting to the sparse data of Class Y while still maintaining good separation for the densely populated Class X.\n",
    "\n",
    "Advantages Over Other Methods:\n",
    "ArcFace: Treats all classes equally, applying the same margin to both overrepresented and underrepresented classes, which can lead to overfitting for smaller classes.\n",
    "AdaptiveFace: Adjusts the margin based on class difficulty, but does so at a class level, not a sample level. While it helps underrepresented classes, it doesn’t provide the per-sample flexibility that FuzzyArcFace offers.\n",
    "UniFace: Uses feature normalization but doesn’t address the specific needs of underrepresented classes with dynamic margin adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116fc74-3f1d-4e65-b57e-c11d49bdd6bb",
   "metadata": {},
   "source": [
    "Conclusion: Why These Features Make FuzzyArcFace Superior\n",
    "The combination of adaptive margin control and its ability to handle imbalanced data gives FuzzyArcFace a significant advantage over other algorithms in real-world face recognition tasks, particularly when the data is noisy or imbalanced. These features allow FuzzyArcFace to:\n",
    "\n",
    "Generalize better to difficult or noisy samples without forcing all data to fit into a rigid decision boundary.\n",
    "Handle underrepresented classes by applying smaller, dynamic margins, preventing overfitting and improving classification accuracy for these classes.\n",
    "In challenging face recognition scenarios, such as surveillance footage, low-quality images, and imbalanced datasets, these features ensure that FuzzyArcFace remains flexible, making it superior in terms of both accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57ec407-25f4-42b3-9f48-a5db8f154ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ffb618-0451-4a61-8817-33a1e22235e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33895d90-a3c8-4486-832e-1d1eef27373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import tarfile\n",
    "import zipfile\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3817dc52-227d-42cb-b9f3-8eedf4736750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5ee325-4bf4-4316-86a3-10e8819fdc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3c5f34-bf57-4ab5-8853-c0cfbd50fe67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy\n",
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967a1f8-5a26-4af2-a487-bfb9503c6ae7",
   "metadata": {},
   "source": [
    "CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf5a142-329e-41ef-8e23-f342a5ebd76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "#lfw_dataset_path = 'lfw'\n",
    "#pairs_file = 'lfw_test_pair_short.txt'\n",
    "#lfw_path = '/home/rapids/notebooks/data/storage/slima/lfw/lfw'\n",
    "\n",
    "# Common directory path\n",
    "#base_path = '/home/rapids/notebooks/data/storage/slima/DATABASES'\n",
    "\n",
    "base_path = '/home/rapids/notebooks/slima/DATABASES'\n",
    "model_save_path = '/home/rapids/notebooks/slima'\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 100\n",
    "#num_classes = 10 # No of images to upload in the dataset\n",
    "num_workers=8\n",
    "#normalization size in pixels\n",
    "resizex=112\n",
    "resizey= 112\n",
    "\n",
    "embedding_size=768\n",
    "train_ratio=0.99\n",
    "\n",
    "#Arcface and Fuzzy Arcface m, tau  and s\n",
    "margin=0.5\n",
    "scale_s=64\n",
    "tauparameter=0.1\n",
    "\n",
    "#SDG optimizer related for Arcface and FuzzyArcface\n",
    "learning_rate= 0.01\n",
    "momentum= 0.9\n",
    "weight_decay= 5e-4\n",
    "\n",
    "#Adaface\n",
    "h_margin=0.33\n",
    "#Adaptiveface\n",
    "lambda_margin=0.01\n",
    "\n",
    "#num classes lfw\n",
    "num_classes=5749"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408de8b-fc4b-4372-9ff7-5aaf74973abf",
   "metadata": {},
   "source": [
    "GPU AVAILABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "720f0ffc-bf58-46c5-9784-0a4737da9c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14fff1588bd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)  # Set the seed for PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27d09b2d-2685-4606-a403-8dd9285fa97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 GPUs available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"{torch.cuda.device_count()} GPUs available\")\n",
    "else:\n",
    "    print(\"Single GPU or no GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7d994d0-857f-4fb5-a1c1-63f10e3e3b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc54241-0e5e-46ae-a482-bdfe07676004",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b92811-7d8d-4346-970d-c7dff3e5c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If using CUDA (PyTorch)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # if you are using multi-GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2617e7d-3404-4eac-ae38-a1c07cc3d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Set up available GPUs and seed for reproducibility\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "559ff1ec-8a4b-4510-863b-e7c1e78c62db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rapids/notebooks/slima'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc92d51-0876-4db8-a0c7-d45240209322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Define image transforms (with normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26956736-1fa0-4f06-bc3e-d303b0703a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ----------------- Data Extraction -------------------\n",
    "\n",
    "# Paths to the archives and where to extract them\n",
    "lfw_tgz_path = os.path.join(base_path, 'lfw.tgz')\n",
    "lfw_extract_path = os.path.join(base_path, 'extracted', 'lfw/lfw')\n",
    "\n",
    "# cfp_tar_path = os.path.join(base_path, 'CFP.tar')\n",
    "# cfp_extract_path = os.path.join(base_path, 'extracted', 'Data/Images')\n",
    "\n",
    "# jaffedbase_tar_path = os.path.join(base_path, 'jaffedbase.tar')\n",
    "# jaffedbase_extract_path = os.path.join(base_path, 'extracted', 'jaffedbase/jaffedbase')\n",
    "\n",
    "# calfw_zip_path = os.path.join(base_path, 'calfw.zip')\n",
    "# calfw_extract_path = os.path.join(base_path, 'extracted', 'calfw/calfw')\n",
    "\n",
    "# cplfw_zip_path = os.path.join(base_path, 'cplfw.zip')\n",
    "# cplfw_extract_path = os.path.join(base_path, 'extracted', 'cplfw/cplfw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2ae5d3d-4ec1-4b4f-9ff3-4aed8651d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract tar files (CFP, Jaffedbase, LFW)\n",
    "for tar_path, extract_path in [#(cfp_tar_path, cfp_extract_path), \n",
    "                               #(jaffedbase_tar_path, jaffedbase_extract_path),\n",
    "                               (lfw_tgz_path, lfw_extract_path)]:\n",
    "    if not os.path.exists(extract_path):\n",
    "        with tarfile.open(tar_path, 'r') as tar:\n",
    "            tar.extractall(extract_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e67cc281-a2aa-4991-9faf-75154095a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract zip files (CALFW, CPLFW)\n",
    "# for zip_path, extract_path in [(calfw_zip_path, calfw_extract_path), \n",
    "#                                (cplfw_zip_path, cplfw_extract_path)]:\n",
    "#     if not os.path.exists(extract_path):\n",
    "#         with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#             zip_ref.extractall(extract_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d812dfc-a212-457b-be50-42038c4bf252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FlatDirectoryImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading images from a directory where images\n",
    "    are stored in subfolders representing class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        \n",
    "        # Walk through the dataset directory and gather images and labels\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.jpeg', '.png', '.tiff')):\n",
    "                    img_path = os.path.join(root, file)\n",
    "                    self.image_paths.append(img_path)\n",
    "                    \n",
    "                    # Extract class label from the folder name (person's name)\n",
    "                    class_name = os.path.basename(os.path.dirname(img_path))\n",
    "                    \n",
    "                    # If the class hasn't been seen before, add it to class_to_idx mapping\n",
    "                    if class_name not in self.class_to_idx:\n",
    "                        self.class_to_idx[class_name] = len(self.class_to_idx)\n",
    "                    \n",
    "                    # Assign the numerical label based on class_to_idx mapping\n",
    "                    label = self.class_to_idx[class_name]\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90d33b11-6e93-406a-92b6-f44c04c5d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----------------- Loading Training Dataset (LFW) -------------------\n",
    "\n",
    "train_dataset = FlatDirectoryImageDataset(lfw_extract_path, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eea9a6a-355d-4e72-911a-6342627d841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # Assuming you're using ImageFolder or a custom dataset with a `targets` attribute\n",
    "# class_counts = Counter(train_dataset.labels)\n",
    "# print(f\"Class distribution: {class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f6531d8-498f-4850-b127-402dfcf7b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataLoader with num_workers and pin_memory for GPU efficiency\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, \n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e815f1ff-e882-421f-956b-04ed5e9f0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_datasets = {\n",
    "#     'CPLFW': FlatDirectoryImageDataset(os.path.join(cplfw_extract_path, 'aligned images'), transform),\n",
    "#     'CALFW': FlatDirectoryImageDataset(os.path.join(calfw_extract_path, 'aligned images'), transform),\n",
    "#     'JEFF': FlatDirectoryImageDataset(jaffedbase_extract_path, transform),\n",
    "#     'CFP': ImageFolder(os.path.join(cfp_extract_path, 'Data/Images'), transform),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61c2fe0c-479f-4932-9c98-35a74edc1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # DataLoader with num_workers and pin_memory for efficient loading\n",
    "# test_loaders = {name: DataLoader(ds, batch_size=32, shuffle=False, \n",
    "#                                  num_workers=num_workers, pin_memory=True) for name, ds in test_datasets.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07754a0d-c38f-4fa8-8321-7e9a50cc900f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Iterate over each test loader and print out some samples\n",
    "# for name, loader in test_loaders.items():\n",
    "#     print(f\"Checking data from {name} dataset:\")\n",
    "    \n",
    "#     # Get one batch of data\n",
    "#     data_iter = iter(loader)\n",
    "    \n",
    "#     # Try fetching one batch\n",
    "#     try:\n",
    "#         images, labels = next(data_iter)  # Get a batch of images and labels\n",
    "#         print(f\"Sample batch from {name}:\")\n",
    "#         print(f\"Images shape: {images.shape}\")  # Check the shape of the images\n",
    "#         print(f\"Labels: {labels[:5]}\")  # Print the first 5 labels\n",
    "#         print(f\"Number of samples in this batch: {len(images)}\\n\")\n",
    "#     except StopIteration:\n",
    "#         print(f\"No data found in the {name} loader.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7554b5bb-079a-405b-9dd6-18a2e22549b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Define the iResNet100 architecture\n",
    "class iResNet100(nn.Module):\n",
    "    def __init__(self, num_classes=num_classes):  # LFW classes\n",
    "        super(iResNet100, self).__init__()\n",
    "        self.model = models.resnet101(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88432021-7444-4628-b4d2-365f5bf17eac",
   "metadata": {},
   "source": [
    "Define all loss classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ef6f1e7-737d-45df-acb6-62f84c1c820e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FuzzyArcFaceLoss(nn.Module):\n",
    "#     def __init__(self, in_features=embedding_size, out_features=num_classes, s=scale_s, m=margin, tau=tauparameter, easy_margin=False):\n",
    "#         super(FuzzyArcFaceLoss, self).__init__()\n",
    "#         self.in_features = in_features\n",
    "#         self.out_features = out_features\n",
    "#         self.s = s\n",
    "#         self.m = m\n",
    "#         self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "#         nn.init.xavier_uniform_(self.weight)  # Initialize the weights\n",
    "#         self.tau = tau\n",
    "#         self.easy_margin = easy_margin\n",
    "\n",
    "#     def forward(self, input, label):\n",
    "        \n",
    "#         #cosine = F.linear(F.normalize(input), F.normalize(self.weight.to(device)))\n",
    "#         cosine = F.linear(F.normalize(input), F.normalize(self.weight.to(device)).T)\n",
    "\n",
    "#         # Fuzzy membership based on cosine, values could be -1 to 1\n",
    "#         fuzzy_membership = cosine\n",
    "#         # Keep values greater than 0 and greater than tau, and less than or equal to 1\n",
    "#         mask = torch.logical_and(torch.logical_and(fuzzy_membership > 0, fuzzy_membership >= self.tau), fuzzy_membership <= 1)\n",
    "#         # Apply mask\n",
    "#         fuzzy_membership = torch.where(mask, fuzzy_membership, torch.ones_like(fuzzy_membership))\n",
    "#         # Adjust margin\n",
    "#         m_adjusted = self.m * fuzzy_membership\n",
    "        \n",
    "#         sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        \n",
    "#         # Compute phi with adjusted m\n",
    "#         cos_m_adjusted = torch.cos(m_adjusted)\n",
    "#         sin_m_adjusted = torch.sin(m_adjusted)\n",
    "#         phi = cosine * cos_m_adjusted - sine * sin_m_adjusted\n",
    "\n",
    "#         if self.easy_margin:\n",
    "#             phi = torch.where(cosine > 0, phi, cosine)\n",
    "#         else:\n",
    "#             phi = torch.where(cosine > torch.cos(torch.tensor(math.pi) - m_adjusted), phi, cosine - torch.sin(torch.tensor(math.pi) - m_adjusted) * m_adjusted)\n",
    "\n",
    "#         one_hot = torch.zeros(cosine.size(), device=input.device)\n",
    "#         one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "#         output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "#         output *= self.s\n",
    "        \n",
    "#         print(f\"Output shape: {output.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "#         #one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "#         #fuzzy_membership = self.compute_fuzzy_membership(logits, labels)\n",
    "#         #adjusted_margin = self.margin * fuzzy_membership\n",
    "#         #logits_with_margin = cosine_sim - one_hot * adjusted_margin\n",
    "#         #scaled_logits = self.scale * logits_with_margin\n",
    "#         loss = F.cross_entropy(output, labels)\n",
    "        \n",
    "        \n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5a2f3c3-399d-4983-bc73-fa7842a11f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyArcFaceLoss(nn.Module):\n",
    "    def __init__(self, in_features=768, out_features=num_classes, s=scale_s, m=margin, tau=tauparameter, easy_margin=False):\n",
    "        super(FuzzyArcFaceLoss, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        # Note that weight should map from in_features (input embedding size) to out_features (number of classes)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))  # Shape (num_classes, embedding_size)\n",
    "        nn.init.xavier_uniform_(self.weight)  # Initialize the weights\n",
    "        self.tau = tau\n",
    "        self.easy_margin = easy_margin\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # Normalize input and weight, then compute cosine similarity\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight.to(input.device)).T)  # Shape (batch_size, num_classes)\n",
    "\n",
    "        # Fuzzy membership based on cosine similarity\n",
    "        fuzzy_membership = cosine\n",
    "        # Apply the mask\n",
    "        mask = torch.logical_and(torch.logical_and(fuzzy_membership > 0, fuzzy_membership >= self.tau), fuzzy_membership <= 1)\n",
    "        fuzzy_membership = torch.where(mask, fuzzy_membership, torch.ones_like(fuzzy_membership))\n",
    "        \n",
    "        # Adjust margin\n",
    "        m_adjusted = self.m * fuzzy_membership\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "\n",
    "        # Compute phi with adjusted margin\n",
    "        cos_m_adjusted = torch.cos(m_adjusted)\n",
    "        sin_m_adjusted = torch.sin(m_adjusted)\n",
    "        phi = cosine * cos_m_adjusted - sine * sin_m_adjusted\n",
    "\n",
    "        # Apply easy margin\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > torch.cos(torch.tensor(math.pi) - m_adjusted), phi, cosine - torch.sin(torch.tensor(math.pi) - m_adjusted) * m_adjusted)\n",
    "\n",
    "        # One-hot encoding for labels\n",
    "        one_hot = torch.zeros(cosine.size(), device=input.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "\n",
    "        # Calculate the final output (logits) based on one-hot encoding\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        # Return the logits for cross-entropy loss outside the function\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "095cba93-28cd-4249-9112-a87510bb7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ArcFace\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, scale=64.0, margin=0.5):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        cosine_sim = F.normalize(logits, dim=1)\n",
    "        one_hot = torch.zeros_like(cosine_sim)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "        cosine_sim_with_margin = cosine_sim - one_hot * self.margin\n",
    "        scaled_logits = self.scale * cosine_sim_with_margin\n",
    "        loss = F.cross_entropy(scaled_logits, labels)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8ff675f-b14c-4d50-8588-c22c7645c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaptiveFace\n",
    "class AdaptiveFaceLoss(nn.Module):\n",
    "    def __init__(self, scale=64.0, base_margin=0.5, lambda_reg=0.001):\n",
    "        super(AdaptiveFaceLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.base_margin = base_margin\n",
    "        self.lambda_reg = lambda_reg\n",
    "    \n",
    "    def compute_adaptive_margin(self, labels, class_counts):\n",
    "        max_count = class_counts.max().float()\n",
    "        class_weights = max_count / class_counts.float()\n",
    "        class_margins = self.base_margin * class_weights[labels]\n",
    "        return class_margins\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "        class_counts = torch.zeros(logits.size(1), device=logits.device)\n",
    "        #class_counts[unique_labels] = counts\n",
    "        class_counts[unique_labels] = counts.float()\n",
    "\n",
    "        \n",
    "        cosine_sim = F.normalize(logits, dim=1)\n",
    "        one_hot = torch.zeros_like(cosine_sim)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        adaptive_margins = self.compute_adaptive_margin(labels, class_counts)\n",
    "        margin_per_class = one_hot * adaptive_margins.view(-1, 1)\n",
    "        logits_with_margin = cosine_sim - margin_per_class\n",
    "        scaled_logits = self.scale * logits_with_margin\n",
    "        loss = F.cross_entropy(scaled_logits, labels)\n",
    "        \n",
    "        regularization_loss = self.lambda_reg * torch.mean(adaptive_margins)\n",
    "        total_loss = loss + regularization_loss\n",
    "        return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e307bc45-09d3-4193-b81d-446eed195a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VPL (Virtual Prototypical Learning)\n",
    "class VPLLoss(nn.Module):\n",
    "    def __init__(self, scale=64.0, margin=0.5, alpha=0.1):\n",
    "        super(VPLLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compute_virtual_prototypes(self, logits, noise_factor=0.1):\n",
    "        noise = torch.randn_like(logits) * noise_factor\n",
    "        virtual_prototypes = logits + noise\n",
    "        return virtual_prototypes\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        cosine_sim_real = F.normalize(logits, dim=1)\n",
    "        one_hot = torch.zeros_like(cosine_sim_real)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        virtual_logits = self.compute_virtual_prototypes(logits)\n",
    "        cosine_sim_virtual = F.normalize(virtual_logits, dim=1)\n",
    "        \n",
    "        real_logits_with_margin = cosine_sim_real - one_hot * self.margin\n",
    "        virtual_logits_with_margin = cosine_sim_virtual - one_hot * self.margin\n",
    "        \n",
    "        scaled_real_logits = self.scale * real_logits_with_margin\n",
    "        scaled_virtual_logits = self.scale * virtual_logits_with_margin\n",
    "        \n",
    "        combined_logits = scaled_real_logits + self.alpha * scaled_virtual_logits\n",
    "        loss = F.cross_entropy(combined_logits, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33a1af54-7d53-4342-9940-606e0f53179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SphereFace2\n",
    "class SphereFace2Loss(nn.Module):\n",
    "    def __init__(self, scale=64.0, margin=1.35):\n",
    "        super(SphereFace2Loss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        cosine_sim = F.normalize(logits, dim=1)\n",
    "        one_hot = torch.zeros_like(cosine_sim)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "        logits_with_margin = cosine_sim * (self.margin * one_hot)\n",
    "        scaled_logits = self.scale * logits_with_margin\n",
    "        loss = F.cross_entropy(scaled_logits, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffcefad1-1834-48df-9114-a1566a2f91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UniFace\n",
    "class UniFaceLoss(nn.Module):\n",
    "    def __init__(self, scale=64.0, margin=0.5):\n",
    "        super(UniFaceLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        cosine_sim = F.normalize(logits, dim=1)\n",
    "        one_hot = torch.zeros_like(cosine_sim)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "        logits_with_margin = cosine_sim - one_hot * self.margin\n",
    "        scaled_logits = self.scale * logits_with_margin\n",
    "        loss = F.cross_entropy(scaled_logits, labels)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71c3f0d1-0f51-4412-a8f3-9075aa316248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniTSFace\n",
    "class UniTSFaceLoss(nn.Module):\n",
    "    def __init__(self, scale=64.0, margin=0.5, temporal_weight=0.5):\n",
    "        super(UniTSFaceLoss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.temporal_weight = temporal_weight\n",
    "\n",
    "    def compute_temporal_logits(self, features, time_step):\n",
    "        print(features.shape)\n",
    "        print(time_step.shape)\n",
    "        logits = F.normalize(features @ time_step.mT, dim=1)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, features_t, features_t1, labels):\n",
    "        logits_t = self.compute_temporal_logits(features_t, features_t)\n",
    "        logits_t1 = self.compute_temporal_logits(features_t1, features_t1)\n",
    "        \n",
    "        cosine_sim_t = F.normalize(logits_t, dim=1)\n",
    "        cosine_sim_t1 = F.normalize(logits_t1, dim=1)\n",
    "        \n",
    "        one_hot = torch.zeros_like(cosine_sim_t)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        logits_t_with_margin = cosine_sim_t - one_hot * self.margin\n",
    "        logits_t1_with_margin = cosine_sim_t1 - one_hot * self.margin\n",
    "        \n",
    "        scaled_logits_t = self.scale * logits_t_with_margin\n",
    "        scaled_logits_t1 = self.scale * logits_t1_with_margin\n",
    "        \n",
    "        loss_t = F.cross_entropy(scaled_logits_t, labels)\n",
    "        loss_t1 = F.cross_entropy(scaled_logits_t1, labels)\n",
    "        \n",
    "        temporal_consistency_loss = F.mse_loss(cosine_sim_t, cosine_sim_t1)\n",
    "        total_loss = loss_t + loss_t1 + self.temporal_weight * temporal_consistency_loss\n",
    "        return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7d98f35-84c3-4ce7-8eaa-dabededb2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Define optimizer and training function\n",
    "def get_optimizer(model, lr=0.01, momentum=0.9):\n",
    "    return SGD(model.parameters(), lr=lr, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd745d8-c682-4f30-99f0-13e1a0f151dc",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b29ddfef-fc35-41d7-80f8-5aacba4d1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list for storing model file names\n",
    "model_filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95ebfec2-00a6-428a-9998-db618fe826d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=num_epochs, loss_name=\"\"):\n",
    "    model.train()  # Set the model to training mode\n",
    "    j=0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Debug: Check labels and inputs\n",
    "            #print(f\"Labels: {labels[:5]}\")  # Check the first few labels\n",
    "            #print(f\"Inputs shape: {inputs.shape}\")\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            \n",
    "           # print(f\"Model outputs (first 5): {outputs[:5]}\")  # Check model output\n",
    "\n",
    "            if loss_name==\"FuzzyArcFace\":\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "            \n",
    "            #print(f\"Loss value: {loss.item()}\")  # Print loss\n",
    "            \n",
    "            loss.backward()  # Backpropagation\n",
    "            \n",
    "            \n",
    "            # Check gradients of one layer\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.requires_grad and param.grad is not None:\n",
    "            #         print(f\"Gradients of {name}: {param.grad[:5]}\")  # Print the first few gradients\n",
    "            #         break\n",
    "                    \n",
    "            \n",
    "            optimizer.step()  # Update model parameters\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            j+=1\n",
    "            #print(j)\n",
    "            if j % 10 == 0:  # Print every 10 steps for better monitoring\n",
    "                print(f\"Step {j}, Loss: {loss.item():.4f}\")\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Save the model with date and loss function in the name\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    #now = \"2024-09-30\"  \n",
    "    model_name = f\"{loss_name}_{now}.pth\"\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(f\"Model saved as {model_name}\")\n",
    "    # Append the saved filename to the list for future loading\n",
    "    #model_filenames.append(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc93e76-463d-49d5-acc2-251f18e775e2",
   "metadata": {},
   "source": [
    "Train all models using different loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ac7f70f-df9d-47ce-b350-a8a7dd3ab083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Train all models using different loss functions\n",
    "loss_functions = {\n",
    "    \"FuzzyArcFace\": FuzzyArcFaceLoss(),\n",
    "    # \"ArcFace\": ArcFaceLoss(),\n",
    "    # \"AdaptiveFace\": AdaptiveFaceLoss(),\n",
    "    # \"VPL\": VPLLoss(),\n",
    "    # \"SphereFace2\": SphereFace2Loss(),\n",
    "    # \"UniFace\": UniFaceLoss(),\n",
    "   # \"UniTSFace\": UniTSFaceLoss(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ffc85-71c0-4593-964f-2567f84dbe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FuzzyArcFace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /home/rapids/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100%|██████████| 171M/171M [00:05<00:00, 35.4MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the trained models\n",
    "modelss = []\n",
    "\n",
    "#models = []\n",
    "now = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#now = \"2024-09-30\"  \n",
    "# Loop over each loss function\n",
    "for loss_name, loss_fn in loss_functions.items():\n",
    "    model_file = os.path.join(model_save_path, f'{loss_name}_{now}.pth')\n",
    "\n",
    "    # Check if the model file exists\n",
    "    if os.path.exists(model_file):\n",
    "        print(f\"Model for {loss_name} already exists. Skipping training...\")\n",
    "        # Load the model from the file\n",
    "        # model = iResNet100()\n",
    "        # model.load_state_dict(torch.load(model_file))\n",
    "        # model = model.to(device)\n",
    "        # modelss.append(model)\n",
    "        model_name = f\"{loss_name}_{now}.pth\"\n",
    "        model_filenames.append(model_name)\n",
    "    else:\n",
    "        print(f\"Training with {loss_name}...\")\n",
    "        model = iResNet100()  # New model for each loss function\n",
    "        model = nn.DataParallel(model) if num_gpus > 1 else model  # DataParallel if multiple GPUs\n",
    "        model = model.to(device)\n",
    "        optimizer = get_optimizer(model)\n",
    "        train_model(model, loss_fn, optimizer, train_loader, num_epochs=num_epochs, loss_name=loss_name)\n",
    "        model_name = f\"{loss_name}_{now}.pth\"\n",
    "        model_filenames.append(model_name)\n",
    "        modelss.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98ac50-2b41-41e7-b90b-2e0ab6d78a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
